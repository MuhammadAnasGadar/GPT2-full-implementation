# Build GPT-2: A Complete Implementation from Scratch

A comprehensive implementation of GPT-2 (Generative Pre-trained Transformer 2) built from scratch using PyTorch. This project demonstrates the complete pipeline of building and training a large language model.

The thorough GPT-2 model implementation packs up all the necessary components for data preprocessing, model training, evaluation, and inference capabilities. The trained 124M model replicates performance close to GPT-3 on the evaluated datasets, showcasing the effectiveness of the implementation.

## üéØ Project Overview

This repository contains a full implementation of GPT-2 with the following key features:

- **Complete GPT-2 Architecture**: Implements the full transformer architecture with causal self-attention, MLP blocks, and layer normalization
- **Training Pipeline**: End-to-end training with distributed data parallel support
- **Evaluation Framework**: HellaSwag evaluation for measuring model performance
- **Data Processing**: FineWeb-Edu dataset processing and tokenization
- **Model Loading**: Support for loading pretrained GPT-2 weights from Hugging Face
- **Text Generation**: Autoregressive text generation with top-k sampling
- **Performance Parity**: The 124M parameter model achieves performance comparable to GPT-3 on various evaluation and validation sets

## üèóÔ∏è Architecture

### Core Components

The implementation follows the original GPT-2 architecture with these key components:

#### 1. **CausalSelfAttention** (`CausalSelfAttention`)
- Multi-head self-attention with causal masking
- Supports configurable number of attention heads
- Implements the query, key, value projection pattern

#### 2. **MLP** (`MLP`)
- Two-layer feedforward network
- Uses GELU activation function with `approximate='tanh'`
- Implements the 4x expansion pattern (hidden_size ‚Üí 4*hidden_size ‚Üí hidden_size)

#### 3. **Transformer Block** (`Block`)
- Combines attention and MLP with residual connections
- Uses pre-norm architecture (LayerNorm before attention/MLP)
- Implements the standard transformer block pattern

#### 4. **GPT Model** (`GPT`)
- Complete GPT-2 model with configurable parameters
- Supports multiple model sizes (124M, 350M, 774M, 1558M parameters)
- Implements weight sharing between input embeddings and output projection
- Includes proper weight initialization

### Model Configurations

| Model | Parameters | Layers | Heads | Embedding Dim |
|-------|------------|--------|-------|---------------|
| GPT-2 | 124M | 12 | 12 | 768 |
| GPT-2 Medium | 350M | 24 | 16 | 1024 |
| GPT-2 Large | 774M | 36 | 20 | 1280 |
| GPT-2 XL | 1558M | 48 | 25 | 1600 |

## üìÅ File Structure

```
Build-gpt-2/
‚îú‚îÄ‚îÄ gpt2.py                 # Main GPT-2 implementation and training loop
‚îú‚îÄ‚îÄ train_gpt2.py          # Standalone training script
‚îú‚îÄ‚îÄ hellaswag.py           # HellaSwag evaluation framework
‚îú‚îÄ‚îÄ fineweb.py             # FineWeb-Edu dataset processing
‚îú‚îÄ‚îÄ fineweb.ipynb          # Jupyter notebook for data processing
‚îú‚îÄ‚îÄ play.ipynb             # Interactive notebook for experimentation
‚îú‚îÄ‚îÄ input.txt              # Sample text data (TinyShakespeare)
‚îú‚îÄ‚îÄ edu_fineweb10B/        # Processed dataset directory
‚îú‚îÄ‚îÄ hellaswag/             # HellaSwag evaluation data
‚îú‚îÄ‚îÄ log/                   # Training logs and checkpoints
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ conda.yaml            # Conda environment specification
‚îî‚îÄ‚îÄ README.md             # This file
```

## üöÄ Quick Start

### Prerequisites

- Python 3.9+
- PyTorch 2.0+
- CUDA (for GPU training)
- 16GB+ RAM (for 124M model)

### Installation

1. **Clone the repository:**
```bash
git clone <repository-url>
cd Build-gpt-2
```

2. **Create conda environment:**
```bash
conda env create -f conda.yaml
conda activate build-gpt
```

3. **Or install with pip:**
```bash
pip install -r requirements.txt
```

### Basic Usage

#### 1. **Process FineWeb-Edu Dataset**
```bash
python fineweb.py
```
This downloads and tokenizes the FineWeb-Edu dataset, creating shards in `edu_fineweb10B/`.

#### 2. **Train GPT-2 from Scratch**
```bash
# Single GPU training
python gpt2.py

# Multi-GPU training (8 GPUs)
torchrun --standalone --nproc_per_node=8 gpt2.py
```

#### 3. **Load Pretrained Model**
```python
from gpt2 import GPT
model = GPT.from_pretrained("gpt2")  # Load GPT-2 (124M)
```

#### 4. **Evaluate on HellaSwag**
```bash
python hellaswag.py -m gpt2 -d cuda
```

## üîß Key Features

### 1. **Distributed Training**
- Supports DistributedDataParallel (DDP) for multi-GPU training
- Automatic device detection (CUDA, MPS, CPU)
- Gradient accumulation for large effective batch sizes
- Learning rate scheduling with warmup and cosine decay

### 2. **Data Loading**
- Efficient sharded data loading from `edu_fineweb10B/`
- Supports train/validation splits
- Automatic shard rotation for continuous training
- Memory-efficient token processing

### 3. **Evaluation Framework**
- **HellaSwag Evaluation**: Measures commonsense reasoning
- **Completion-style Evaluation**: Treats multiple-choice as text completion
- **Autoregressive Loss Calculation**: Evaluates model confidence
- **Distributed Evaluation**: Supports multi-GPU evaluation

### 4. **Text Generation**
- **Top-k Sampling**: Configurable sampling strategy
- **Temperature Control**: Adjustable randomness
- **Batch Generation**: Generate multiple sequences simultaneously
- **Autoregressive Generation**: Token-by-token generation

## üìä Training Configuration

### Hyperparameters
- **Batch Size**: 524,288 tokens (0.5M tokens per step)
- **Sequence Length**: 1024 tokens
- **Learning Rate**: 6e-4 with cosine decay
- **Weight Decay**: 0.1
- **Gradient Clipping**: 1.0
- **Warmup Steps**: 715
- **Total Steps**: 19,073 (~1 epoch on 10B tokens)

### Optimization
- **Optimizer**: AdamW with fused implementation
- **Mixed Precision**: bfloat16 for training
- **Gradient Accumulation**: Automatic based on batch size
- **Learning Rate Schedule**: Linear warmup + cosine decay

## üß™ Evaluation Results

### HellaSwag Performance
The model is evaluated on HellaSwag, a commonsense reasoning benchmark:

| Model | Accuracy (Completion Style) | Baseline (Multiple Choice) |
|-------|----------------------------|---------------------------|
| GPT-2 (124M) | 29.55% | 31.14% |
| GPT-2 XL (1558M) | 48.93% | 50.89% |

### Performance Comparison with GPT-3
Remarkably, the 124M parameter GPT-2 model demonstrates performance that closely approaches GPT-3 levels on various evaluation and validation sets. 

### Training Metrics
- **Validation Loss**: Monitored every 250 steps
- **HellaSwag Accuracy**: Evaluated every 250 steps
- **Text Generation**: Sample generation every 250 steps
- **Checkpointing**: Model saved every 5000 steps

## üîç Technical Details

### 1. **Autoregressive Language Modeling**
The model uses the standard next-token prediction objective:
```
P(x‚ÇÅ, x‚ÇÇ, ..., x‚Çô) = ‚àè·µ¢ P(x·µ¢ | x‚ÇÅ, ..., x·µ¢‚Çã‚ÇÅ)
```

### 2. **Data Processing**
- **Tokenization**: GPT-2 BPE tokenizer via `tiktoken`
- **Sharding**: 100M tokens per shard for efficient loading
- **Memory Management**: uint16 storage for token indices
- **Document Separation**: `<|endoftext|>` token delimiters

### 3. **Model Architecture Details**
- **Position Embeddings**: Learnable position embeddings
- **Layer Normalization**: Pre-norm architecture
- **Weight Sharing**: Input embeddings = output projection weights
- **Initialization**: Proper weight initialization following GPT-2 paper

### 4. **HellaSwag Evaluation**
- Converts multiple-choice to completion task
- Uses autoregressive loss for ranking completions
- Implements length-normalized scoring
- Supports distributed evaluation across GPUs


## üéÆ Interactive Notebooks

### `play.ipynb`
Interactive notebook for:
- Model weight visualization
- Text generation experiments
- Training progress analysis
- Architecture exploration

## üöÄ Advanced Usage

### Custom Model Configuration
```python
from gpt2 import GPT, GPTConfig

# Custom configuration
config = GPTConfig(
    block_size=2048,    # Longer context
    vocab_size=50304,   # Custom vocabulary
    n_layer=16,         # More layers
    n_head=16,          # More attention heads
    n_embd=1024         # Larger embedding dimension
)

model = GPT(config)
```

### Custom Training Loop
```python
# Load your own data
train_loader = DataLoaderLite(B=32, T=512, process_rank=0, num_processes=1, split="train")

# Custom training parameters
optimizer = model.configure_optimizers(weight_decay=0.01, learning_rate=1e-4, device_type="cuda")

# Training loop
for step in range(max_steps):
    x, y = train_loader.next_batch()
    logits, loss = model(x, y)
    loss.backward()
    optimizer.step()
```

## üêõ Troubleshooting

### Common Issues

1. **CUDA Out of Memory**
   - Reduce batch size (`B`)
   - Use gradient accumulation
   - Enable mixed precision training

2. **Slow Training**
   - Use multiple GPUs with DDP
   - Enable `torch.compile()` (experimental)
   - Use fused AdamW optimizer

3. **Data Loading Issues**
   - Ensure `edu_fineweb10B/` directory exists
   - Run `python fineweb.py` to process data
   - Check file permissions

## üìö References

- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - Original GPT-2 paper
- [HellaSwag: A Challenge Dataset for Commonsense Reasoning](https://arxiv.org/abs/1905.07830) - Evaluation benchmark
- [FineWeb-Edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu) - Training dataset
- [nanoGPT](https://github.com/karpathy/nanoGPT) - Inspiration for this implementation

## üìÑ License

This project is for educational purposes. Please refer to the original GPT-2 license for commercial use.


